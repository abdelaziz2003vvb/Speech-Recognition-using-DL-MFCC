
# Speech Recognition Using Deep Learning

A Convolutional Neural Network (CNN) based speaker recognition system that identifies speakers from audio files using MFCC (Mel-Frequency Cepstral Coefficients) features.

## ğŸ“‹ Overview

This project implements a deep learning model for speaker recognition using:
- **Feature Extraction**: MFCC features extracted from audio files
- **Model Architecture**: CNN with convolutional and pooling layers
- **Classification**: Multi-class classification using softmax activation

## ğŸ—ï¸ Architecture Pipeline

```
Audio file (.wav)
    â†“
Feature extraction (MFCCs / Spectrogram)
    â†“
Deep Learning Model (CNN or LSTM)
    â†“
Softmax Layer â†’ predicts speaker ID
```

## ğŸ”§ Model Architecture

The CNN model consists of:
- **Input Layer**: 40 x 200 MFCC features
- **Conv2D Layer 1**: 32 filters, 3x3 kernel, ReLU activation
- **MaxPooling2D Layer 1**: 2x2 pool size
- **Conv2D Layer 2**: 64 filters, 3x3 kernel, ReLU activation
- **MaxPooling2D Layer 2**: 2x2 pool size
- **Flatten Layer**: Converts 2D features to 1D
- **Dense Layer**: 128 neurons, ReLU activation
- **Dropout Layer**: 0.3 dropout rate
- **Output Layer**: Softmax activation for speaker classification

**Total Parameters**: 3,165,575 (12.08 MB)

## ğŸ“Š Performance

- **Training Accuracy**: ~98.46% (Epoch 5)
- **Validation Accuracy**: ~98.67% (Epoch 5)
- **Test Accuracy**: **98.67%**

### Training History

| Epoch | Training Acc | Training Loss | Val Acc | Val Loss |
|-------|-------------|---------------|---------|----------|
| 1     | 69.95%      | 1.3769        | 98.67%  | 0.0589   |
| 2     | 97.18%      | 0.1016        | 98.80%  | 0.0379   |
| 3     | 99.08%      | 0.0386        | 98.80%  | 0.0298   |
| 4     | 98.97%      | 0.0352        | 98.40%  | 0.0491   |
| 5     | 98.46%      | 0.0381        | 98.67%  | 0.0438   |

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install librosa numpy soundfile scikit-learn tensorflow
```

### Installation

1. Mount Google Drive (if using Google Colab):
```python
from google.colab import drive
drive.mount('/content/drive')
```

2. Extract the dataset:
```python
import zipfile
import os

zip_path = '/content/drive/MyDrive/dataset_speech/archive_dataset.zip'
extract_path = '/content/drive/MyDrive/dataset_speech/'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
```

### Dataset Structure

```
dataset_speech/
â””â”€â”€ 16000_pcm_speeches/
    â”œâ”€â”€ Speaker_1/
    â”‚   â”œâ”€â”€ 0.wav
    â”‚   â”œâ”€â”€ 1.wav
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ Speaker_2/
    â”‚   â”œâ”€â”€ 0.wav
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...
```

**Dataset Statistics**: 7,507 audio samples

## ğŸ’» Usage

### Feature Extraction

```python
import librosa
import numpy as np

def extract_features(file_path, max_pad_len=200):
    audio, sample_rate = librosa.load(file_path, sr=None, mono=True)
    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    
    # Padding/Truncating to fixed length
    pad_width = max_pad_len - mfccs.shape[1]
    if pad_width > 0:
        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')
    else:
        mfccs = mfccs[:, :max_pad_len]
    
    return mfccs
```

### Training the Model

```python
history = model.fit(
    X_train, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(X_test, y_test)
)
```

### Making Predictions

```python
test_file = "path/to/audio/file.wav"
feature = extract_features(test_file)

if feature is not None:
    prediction = model.predict(feature[np.newaxis, ..., np.newaxis])
    predicted_speaker = le.inverse_transform([np.argmax(prediction)])
    print("Recognized Speaker:", predicted_speaker[0])
```

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ README.md
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ 16000_pcm_speeches/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ speaker_recognition_model.h5
â””â”€â”€ notebooks/
    â””â”€â”€ speech_recognition.ipynb
```

## ğŸ” Key Features

- **Robust Audio Processing**: Handles both standard WAV files and raw PCM format
- **Fallback Mechanism**: Uses FFmpeg for problematic audio files
- **Fixed-Length Features**: MFCC features padded/truncated to 200 frames
- **Data Augmentation Ready**: Architecture supports various audio preprocessing techniques
- **High Accuracy**: Achieves 98.67% test accuracy with minimal epochs

## ğŸ› ï¸ Technologies Used

- **Python 3.12**
- **TensorFlow/Keras**: Deep learning framework
- **Librosa**: Audio processing and feature extraction
- **NumPy**: Numerical computations
- **Scikit-learn**: Data preprocessing and evaluation
- **FFmpeg**: Audio format conversion (fallback)

## ğŸ“ˆ Future Improvements

- [ ] Implement LSTM/GRU layers for temporal feature learning
- [ ] Add data augmentation (pitch shifting, time stretching)
- [ ] Experiment with mel-spectrograms and raw waveforms
- [ ] Deploy as a REST API
- [ ] Add real-time speaker recognition
- [ ] Implement transfer learning with pre-trained models

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¥ Authors

- Your Name - Initial work

## ğŸ™ Acknowledgments

- Dataset source: [Specify dataset source]
- Librosa library for audio processing
- TensorFlow/Keras team for the deep learning framework

---

**Note**: This model was trained on Google Colab with the dataset stored in Google Drive. Adjust paths accordingly for your environment.
```
